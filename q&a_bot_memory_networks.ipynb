{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Embedding, Input, LSTM, Lambda, Reshape, Dense, dot, Activation\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_file = 'data/tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(input_data_file):\n",
    "\n",
    "    counter = 0\n",
    "    data = []\n",
    "    with open(input_data_file) as f:\n",
    "        for line in f:\n",
    "            sid, sline = (line.split(' ', 1))\n",
    "\n",
    "            if int(sid) == 1:\n",
    "                story = []\n",
    "\n",
    "            if '\\t' in line:\n",
    "                q, a, _ = sline.split('\\t')\n",
    "\n",
    "                story_until_now = story.copy()\n",
    "                \n",
    "                if q.strip() and a.strip():\n",
    "                    data.append((story_until_now, q.strip(), a.strip()))\n",
    "            else:\n",
    "                if line.strip():\n",
    "                    story.append(line.strip())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_flag(el):\n",
    "    return isinstance(el, str)\n",
    "\n",
    "def flatten_fn(l):\n",
    "    for el in l:\n",
    "        if flatten_flag(el):\n",
    "            yield el\n",
    "        else:\n",
    "            yield from flatten_fn(el)\n",
    "            \n",
    "def tokenize(unique_sents):\n",
    "    vocab = []\n",
    "    empty = ['', ' ']\n",
    "    for i in unique_sents:\n",
    "        temp = []\n",
    "        l = re.split('(\\W+?)', i)\n",
    "        temp = [el for el in l if el not in empty]\n",
    "        vocab.extend(temp)\n",
    "    return vocab\n",
    "\n",
    "def get_vocab(all_stories):\n",
    "    unique_sents = set(flatten_fn(all_stories))\n",
    "    vocab = set(tokenize(unique_sents))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(all_stories, word2ind, max_story_len, max_query_len):\n",
    "    stories = []\n",
    "    queries = []\n",
    "    answers = []\n",
    "\n",
    "    for s, q, a in all_stories:\n",
    "#         print(s,q,a)\n",
    "        stories.append([[word2ind[w] for w in tokenize([l])] for l in s ])\n",
    "        queries.append([word2ind[w] for w in tokenize([q])])\n",
    "        answers.append([word2ind[w] for w in tokenize([a])])\n",
    "\n",
    "    padded_stories = [pad_sequences(s, max_story_len) for s in stories]\n",
    "    padded_queries = pad_sequences(queries, max_query_len) \n",
    "    \n",
    "    return padded_stories, np.array(padded_queries), np.array(answers)\n",
    "\n",
    "def stack(stories, max_num_story, max_story_len):  \n",
    "    output = np.zeros((len(stories), max_num_story, max_story_len))\n",
    "    for i, s in enumerate(stories):\n",
    "        placeholder = np.zeros((max_num_story-len(s), max_story_len))\n",
    "        s_arr = np.array([np.array(s_1) for s_1 in s])\n",
    "\n",
    "        intmd_arr = np.vstack([s_arr, placeholder])\n",
    "        output[i] = intmd_arr\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(challenge):\n",
    "    input_data_file = 'data/tasks_1-20_v1-2/en-10k/qa1_{0}-supporting-fact_{1}.txt'\n",
    "    \n",
    "    train_data = get_stories(input_data_file.format(challenge, 'train'))\n",
    "    test_data = get_stories(input_data_file.format(challenge, 'test'))\n",
    "    \n",
    "    all_stories = train_data + test_data\n",
    "    \n",
    "    max_num_story = max([len(s) for s,q,a in all_stories])\n",
    "    max_story_len = max([len(tokenize([s1])) for s,q,a in all_stories for s1 in s])\n",
    "    max_query_len = max([len(tokenize([q])) for s,q,a in all_stories])\n",
    "    \n",
    "    vocab = sorted(get_vocab(all_stories))\n",
    "    vocab.insert(0, '<PAD>')\n",
    "#     print(vocab, type(vocab))\n",
    "    word2ind = {w:i for i,w in enumerate(vocab)}\n",
    "        \n",
    "    train_stories, train_queries, train_answers = vectorize_stories(train_data, word2ind, max_story_len, max_query_len)\n",
    "    train_stories = stack(train_stories, max_num_story, max_story_len)\n",
    "    \n",
    "    test_stories, test_queries, test_answers = vectorize_stories(test_data, word2ind, max_story_len, max_query_len)\n",
    "    test_stories = stack(test_stories, max_num_story, max_story_len)\n",
    "    \n",
    "    return (word2ind, vocab, max_num_story, max_story_len, max_query_len,\n",
    "            train_stories, train_queries, train_answers,\n",
    "            test_stories, test_queries, test_answers,\n",
    "            train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(word2ind, vocab, max_num_story, max_story_len, max_query_len,\n",
    "train_stories, train_queries, train_answers,\n",
    "test_stories, test_queries, test_answers,\n",
    "train_data, test_data) = get_data('single')\n",
    "\n",
    "max_num_story, max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data - stories, queries, ans (10000, 10, 8) (10000, 4) (10000, 1)\n",
      "Test data - stories, queries, ans (1000, 10, 8) (1000, 4) (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train data - stories, queries, ans', train_stories.shape, train_queries.shape, train_answers.shape)\n",
    "print('Test data - stories, queries, ans', test_stories.shape, test_queries.shape, test_answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '.', '1', '10', '11', '13', '14', '2', '4', '5', '7', '8', '?', 'Daniel', 'John', 'Mary', 'Sandra', 'Where', 'back', 'bathroom', 'bedroom', 'garden', 'hallway', 'is', 'journeyed', 'kitchen', 'moved', 'office', 'the', 'to', 'travelled', 'went']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model - 1 fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 15\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:71: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:514: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4076: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "input_story_.shape, embedded_story.shape: (?, 10, 8) (?, 10, 15)\n"
     ]
    }
   ],
   "source": [
    "input_story_ = Input(shape=(max_num_story, max_story_len))\n",
    "embedded_story = Embedding(input_dim = vocab_size, output_dim = EMBEDDING_DIM)(input_story_)\n",
    "\n",
    "# treating each story line like a \"bag of words\"\n",
    "embedded_story = Lambda(lambda x: K.sum(x, axis=2))(embedded_story)\n",
    "\n",
    "print(\"input_story_.shape, embedded_story.shape:\", input_story_.shape, embedded_story.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 30)\n",
      "input_query_.shape, embedded_query.shape: (?, 4) (?, 30)\n"
     ]
    }
   ],
   "source": [
    "input_query_ = Input(shape=(max_query_len, ))\n",
    "embedded_query = Embedding(input_dim = vocab_size, output_dim = EMBEDDING_DIM)(input_query_)\n",
    "print(embedded_query.shape)\n",
    "# treating each query like a \"bag of words\"\n",
    "embedded_query = Lambda(lambda x: K.sum(x, axis=1))(embedded_query)\n",
    "\n",
    "print(\"input_query_.shape, embedded_query.shape:\", input_query_.shape, embedded_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_query.shape: (?, 1, 15)\n"
     ]
    }
   ],
   "source": [
    "# for dot product, add dimension to query\n",
    "embedded_query = Reshape((1, EMBEDDING_DIM))(embedded_query)\n",
    "print(\"embedded_query.shape:\", embedded_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 1)\n",
      "(?, 10)\n",
      "(?, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "x = dot([embedded_story, embedded_query], axes=2)\n",
    "print(x.shape)\n",
    "x = Reshape((max_num_story, ))(x)\n",
    "print(x.shape)\n",
    "x = Activation('softmax')(x)\n",
    "weights = Reshape((max_num_story, 1))(x)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 15)\n",
      "(?, 15)\n"
     ]
    }
   ],
   "source": [
    "x = dot([weights, embedded_story], 1)\n",
    "print(x.shape)\n",
    "x = Reshape((EMBEDDING_DIM, ))(x)\n",
    "print(x.shape)\n",
    "ans = Dense(vocab_size, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3259: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model([input_story_, input_query_], ans)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:983: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 2.1188 - acc: 0.1705 - val_loss: 1.8030 - val_acc: 0.1780\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1.7761 - acc: 0.2405 - val_loss: 1.6826 - val_acc: 0.3730\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1.4866 - acc: 0.5207 - val_loss: 1.2579 - val_acc: 0.6320\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 0.7977 - acc: 0.8406 - val_loss: 0.4051 - val_acc: 0.9630\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.2111 - acc: 0.9953 - val_loss: 0.1156 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0707 - acc: 0.9999 - val_loss: 0.0497 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 8.6753e-04 - acc: 1.0000 - val_loss: 8.4385e-04 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 7.0605e-04 - acc: 1.0000 - val_loss: 6.9134e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1389ee4d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_stories, train_queries], train_answers, batch_size=32, epochs=20,\n",
    "         validation_data=([test_stories, test_queries], test_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2word = {v:k for k,v in word2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story:\n",
      "\n",
      "0.00000 \t 1   J o h n   t r a v e l l e d   t o   t h e   o f f i c e .\n",
      "0.00000 \t 2   D a n i e l   w e n t   b a c k   t o   t h e   g a r d e n .\n",
      "0.00000 \t 4   D a n i e l   t r a v e l l e d   t o   t h e   o f f i c e .\n",
      "0.00000 \t 5   J o h n   t r a v e l l e d   t o   t h e   h a l l w a y .\n",
      "0.00002 \t 7   J o h n   j o u r n e y e d   t o   t h e   g a r d e n .\n",
      "0.99997 \t 8   M a r y   m o v e d   t o   t h e   h a l l w a y .\n",
      "question: W h e r e   i s   M a r y ?\n",
      "answer: hallway\n",
      "Predicted answer: hallway\n"
     ]
    }
   ],
   "source": [
    "# Check how we weight each input sentence given a story and question\n",
    "debug_model = Model([input_story_, input_query_], weights)\n",
    "\n",
    "# choose a random story\n",
    "story_idx = np.random.choice(len(train_data))\n",
    "\n",
    "# get weights from debug model\n",
    "i = train_stories[story_idx:story_idx+1]\n",
    "q = train_queries[story_idx:story_idx+1]\n",
    "w = debug_model.predict([i, q]).flatten()\n",
    "\n",
    "story, question, ans_ = train_data[story_idx]\n",
    "print(\"story:\\n\")\n",
    "for j, line in enumerate(story):\n",
    "  print(\"{:1.5f}\".format(w[j]), \"\\t\", \" \".join(line))\n",
    "\n",
    "print(\"question:\", \" \".join(question))\n",
    "print(\"answer:\", ans_)\n",
    "\n",
    "pred = model.predict([i, q])\n",
    "ind = pred[0].argmax()\n",
    "print(\"Predicted answer:\", ind2word[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model - 2 fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_fn(emb_input, axis_=2):\n",
    "    x = Embedding(input_dim = vocab_size, output_dim = EMBEDDING_DIM)(emb_input)\n",
    "    print('Intmd shape:', x.shape)\n",
    "    x = Lambda(lambda x: K.sum(x, axis=axis_))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 8)\n",
      "Intmd shape: (?, 10, 8, 30)\n",
      "input_story_.shape, embedded_story.shape: (?, 10, 8) (?, 10, 30)\n"
     ]
    }
   ],
   "source": [
    "input_story_ = Input(shape=(max_num_story, max_story_len))\n",
    "print(input_story_.shape)\n",
    "embedded_story = embedding_fn(input_story_, 2)\n",
    "\n",
    "print(\"input_story_.shape, embedded_story.shape:\", input_story_.shape, embedded_story.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4)\n",
      "Intmd shape: (?, 4, 30)\n",
      "input_query_.shape, embedded_query.shape: (?, 4) (?, 30)\n"
     ]
    }
   ],
   "source": [
    "input_query_ = Input(shape=(max_query_len, ))\n",
    "print(input_query_.shape)\n",
    "embedded_query = embedding_fn(input_query_, 1)\n",
    "\n",
    "print(\"input_query_.shape, embedded_query.shape:\", input_query_.shape, embedded_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hop(story, query):\n",
    "    \n",
    "    # for dot product, add dimension to query\n",
    "    query = Reshape((1, EMBEDDING_DIM))(query)\n",
    "    x = dot([story, query], axes=2)\n",
    "    x = Reshape((max_num_story, ))(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    weights = Reshape((max_num_story, 1))(x)\n",
    "    \n",
    "    # make new embedding for second calculation\n",
    "    story_2 = embedding_fn(input_story_, 2)\n",
    "    x = dot([weights, story_2], 1)\n",
    "    x = Reshape((EMBEDDING_DIM, ))(x)\n",
    "    x = dense_layer(x)\n",
    "    return x, weights, story_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intmd shape: (?, 10, 8, 30)\n",
      "Intmd shape: (?, 10, 8, 30)\n"
     ]
    }
   ],
   "source": [
    "ans1, weights1, embedded_story = hop(embedded_story, embedded_query)\n",
    "ans2, weights2, _ = hop(embedded_story, embedded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model([input_story_, input_query_], ans2)\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 1s 87us/step - loss: 6.7982e-05 - acc: 1.0000 - val_loss: 1.4263e-05 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 9.1076e-06 - acc: 1.0000 - val_loss: 6.5477e-06 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 5.3164e-06 - acc: 1.0000 - val_loss: 4.6176e-06 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 4.1336e-06 - acc: 1.0000 - val_loss: 3.8769e-06 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 3.6572e-06 - acc: 1.0000 - val_loss: 3.5508e-06 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13acda650>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([train_stories, train_queries], train_answers, batch_size=32, epochs=5,\n",
    "         validation_data=([test_stories, test_queries], test_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story:\n",
      "\n",
      "0.11208 \t 0.00000 \t 1   M a r y   t r a v e l l e d   t o   t h e   k i t c h e n .\n",
      "0.05016 \t 0.00000 \t 2   D a n i e l   t r a v e l l e d   t o   t h e   b e d r o o m .\n",
      "0.06639 \t 0.00021 \t 4   M a r y   t r a v e l l e d   t o   t h e   b a t h r o o m .\n",
      "0.06788 \t 0.00000 \t 5   S a n d r a   t r a v e l l e d   t o   t h e   g a r d e n .\n",
      "0.10584 \t 0.00000 \t 7   D a n i e l   m o v e d   t o   t h e   h a l l w a y .\n",
      "0.17139 \t 0.99979 \t 8   M a r y   j o u r n e y e d   t o   t h e   k i t c h e n .\n",
      "question: W h e r e   i s   M a r y ?\n",
      "answer: kitchen\n",
      "Predicted answer: kitchen\n"
     ]
    }
   ],
   "source": [
    "# Check how we weight each input sentence given a story and question\n",
    "debug_model2 = Model([input_story_, input_query_], [weights1, weights2])\n",
    "\n",
    "# choose a random story\n",
    "story_idx = np.random.choice(len(train_data))\n",
    "\n",
    "# get weights from debug model\n",
    "i = train_stories[story_idx:story_idx+1]\n",
    "q = train_queries[story_idx:story_idx+1]\n",
    "w1, w2 = debug_model2.predict([i, q])\n",
    "w1 = w1.flatten()\n",
    "w2 = w2.flatten()\n",
    "\n",
    "story, question, ans_ = train_data[story_idx]\n",
    "print(\"story:\\n\")\n",
    "for j, line in enumerate(story):\n",
    "  print(\"{:1.5f}\".format(w1[j]), \"\\t\", \"{:1.5f}\".format(w2[j]), \"\\t\", \" \".join(line))\n",
    "\n",
    "print(\"question:\", \" \".join(question))\n",
    "print(\"answer:\", ans_)\n",
    "\n",
    "pred = model2.predict([i, q])\n",
    "ind = pred[0].argmax()\n",
    "print(\"Predicted answer:\", ind2word[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
