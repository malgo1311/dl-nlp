{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Embedding, Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input = []\n",
    "\n",
    "decoder_output = []\n",
    "decoder_input = []\n",
    "\n",
    "counter = 0\n",
    "for line in open('data/spa-eng/spa.txt'):\n",
    "    temp_line = line.split('\\t')\n",
    "    if not temp_line:\n",
    "        continue\n",
    "    \n",
    "    input_ = temp_line[0].strip()\n",
    "    translation = temp_line[1].strip()\n",
    "    \n",
    "    encoder_input.append(input_)\n",
    "    \n",
    "    decoder_input.append('<sos> ' + translation)\n",
    "    decoder_output.append(translation + ' <eos>')\n",
    "    \n",
    "    counter += 1\n",
    "    if counter >= NUM_SAMPLES:\n",
    "        break\n",
    "    \n",
    "len(encoder_input), len(decoder_input), len(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "379\n"
     ]
    }
   ],
   "source": [
    "## Tokenizing Encoder Data\n",
    "en_tokenizer_ = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "en_tokenizer_.fit_on_texts(encoder_input)\n",
    "\n",
    "## Total number of unique words\n",
    "en_word2ind = en_tokenizer_.word_index\n",
    "print(len(en_word2ind))\n",
    "\n",
    "en_sequences = en_tokenizer_.texts_to_sequences(encoder_input)\n",
    "\n",
    "en_NUM_WORDS = min(len(en_word2ind) + 1, MAX_VOCAB_SIZE)\n",
    "print(en_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, [182])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_sequences), en_sequences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n",
      "1015\n"
     ]
    }
   ],
   "source": [
    "dec_tokenizer_ = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "dec_tokenizer_.fit_on_texts(decoder_input + decoder_output)\n",
    "\n",
    "## Total number of unique words\n",
    "dec_word2ind = dec_tokenizer_.word_index\n",
    "print(len(dec_word2ind))\n",
    "\n",
    "dec_out_sequences = dec_tokenizer_.texts_to_sequences(decoder_output)\n",
    "dec_inp_sequences = dec_tokenizer_.texts_to_sequences(decoder_input)\n",
    "\n",
    "dec_NUM_WORDS = min(len(dec_word2ind) + 1, MAX_VOCAB_SIZE)\n",
    "print(dec_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([121, 2], 'Ve. <eos>')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dec_out_sequences[0], decoder_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "## getting max len\n",
    "MAX_ENC_SEQ_LEN = max([len(s) for s in en_sequences])\n",
    "print(MAX_ENC_SEQ_LEN)\n",
    "\n",
    "en_sequences = pad_sequences(en_sequences, maxlen=MAX_ENC_SEQ_LEN) # default padding='pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  41, 242], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sequences[998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "## getting max len\n",
    "MAX_DEC_SEQ_LEN = max([len(s) for s in dec_out_sequences])\n",
    "print(MAX_DEC_SEQ_LEN)\n",
    "\n",
    "dec_inp_sequences = pad_sequences(dec_inp_sequences, maxlen=MAX_DEC_SEQ_LEN, padding='post')\n",
    "dec_out_sequences = pad_sequences(dec_out_sequences, maxlen=MAX_DEC_SEQ_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 121,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_inp_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hoting the target outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to one-hot the targets because cannot use sparse-categorical-crossentropy for list of outputs\n",
    "one_hot_targets = np.zeros((len(dec_out_sequences), MAX_DEC_SEQ_LEN, dec_NUM_WORDS))\n",
    "\n",
    "for i, seq in enumerate(dec_out_sequences):\n",
    "    \n",
    "    for j, word_ind in enumerate(seq):\n",
    "        \n",
    "        one_hot_targets[i, j, word_ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7, 1015)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "for line in open('glove.6B/glove.6B.{0}d.txt'.format(EMBEDDING_DIM)):\n",
    "    temp = line.strip().split(' ')\n",
    "    word = temp[0]\n",
    "    arr = np.asarray(temp[1:], dtype='float16')\n",
    "    word2vec[word] = arr\n",
    "\n",
    "print(len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((en_NUM_WORDS, EMBEDDING_DIM))\n",
    "for k, i in en_word2ind.items():\n",
    "\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(k)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:71: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:514: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4076: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:171: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:178: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ENCODER ###\n",
    "## create the embedding_layer\n",
    "en_embedding_layer = Embedding(input_dim=en_NUM_WORDS, output_dim=EMBEDDING_DIM, weights=[embedding_matrix])\n",
    "\n",
    "en_input = Input(shape=(MAX_ENC_SEQ_LEN, ))\n",
    "# initial_h = Input(shape=(LATENT_DIM, ))\n",
    "# initial_c = Input(shape=(LATENT_DIM, ))\n",
    "\n",
    "x = en_embedding_layer(en_input)\n",
    "\n",
    "lstm1 = LSTM(LATENT_DIM, return_state=True) # return_sequences=True, \n",
    "en_o, en_h, en_c = lstm1(x)#, initial_state = [initial_h, initial_c])\n",
    "\n",
    "en_states = [en_h, en_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DECODER ###\n",
    "dec_embedding_layer = Embedding(input_dim=dec_NUM_WORDS, output_dim=EMBEDDING_DIM)#, weights=[embedding_matrix])\n",
    "\n",
    "dec_input = Input(shape=(MAX_DEC_SEQ_LEN, ))\n",
    "y = dec_embedding_layer(dec_input)\n",
    "\n",
    "lstm2 = LSTM(LATENT_DIM, return_state=True, return_sequences=True)\n",
    "dec_o, _, _ = lstm2(y, initial_state = en_states)\n",
    "\n",
    "dense = Dense(dec_NUM_WORDS, activation='softmax')\n",
    "output = dense(dec_o)\n",
    "\n",
    "model = Model([en_input, dec_input], output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(15)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 1015, (1000, 7, 1015))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_NUM_WORDS, dec_NUM_WORDS, one_hot_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 3, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_ENC_SEQ_LEN, MAX_DEC_SEQ_LEN, len(en_sequences[0]), len(dec_inp_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_sequences), len(dec_inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aishwaryamalgonde/Aishwarya/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/2\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 2.9051 - acc: 0.6102 - val_loss: 2.5791 - val_acc: 0.6514\n",
      "Epoch 2/2\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.8517 - acc: 0.7230 - val_loss: 2.6133 - val_acc: 0.6750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1343e8710>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([en_sequences, dec_inp_sequences],\n",
    "          one_hot_targets,\n",
    "          epochs=2,\n",
    "          batch_size=5,\n",
    "          validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(en_input, en_states)\n",
    "\n",
    "dec_input_single = Input(shape=(1, ))\n",
    "dec_input_single_x = dec_embedding_layer(dec_input_single)\n",
    "\n",
    "initial_h = Input(shape=(LATENT_DIM, ))\n",
    "initial_c = Input(shape=(LATENT_DIM, ))\n",
    "# dec_inp_states = [initial_h, initial_c]\n",
    "\n",
    "dec_op, dec_h, dec_c = lstm2(dec_input_single_x, initial_state = [initial_h, initial_c])\n",
    "# dec_o, dec_h, dec_c = lstm2(dec_input, initial_state = dec_inp_states)\n",
    "# dec_out_states = [dec_h, dec_c]\n",
    "\n",
    "dec_output = dense(dec_op)\n",
    "\n",
    "decoder_model = Model([dec_input_single, initial_h, initial_c], [dec_output, dec_h, dec_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_ind2word = {v:k for k, v in dec_word2ind.items()}\n",
    "len(dec_ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ind2word = {v:k for k, v in en_word2ind.items()}\n",
    "len(en_ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate():\n",
    "    \n",
    "    i = np.random.choice(len(en_sequences))\n",
    "    input_seq = en_sequences[i:i+1]\n",
    "    print('Input seq:', input_seq)\n",
    "    input_text = ' '.join([en_ind2word.get(w) for w in input_seq[0] if w>0])\n",
    "    print('Input seq:', input_text)\n",
    "    \n",
    "    en_output_prediction = encoder_model.predict(input_seq)\n",
    "    h = en_output_prediction[0]\n",
    "    c = en_output_prediction[1]\n",
    "    \n",
    "    sos = dec_word2ind['<sos>']\n",
    "    dec_start = np.array([[sos]])\n",
    "    \n",
    "    eos = dec_word2ind['<eos>']\n",
    "    \n",
    "    generated_output = []\n",
    "    \n",
    "    for _ in range(MAX_DEC_SEQ_LEN):\n",
    "        p, h, c = decoder_model.predict([dec_start, h, c])\n",
    "        \n",
    "        # probabilties\n",
    "        probs = p[0,0]\n",
    "        index = np.argmax(probs)\n",
    "#         print(index)\n",
    "        \n",
    "        # if probabilty of <sos> is high then it prints a warning\n",
    "        if index == sos:\n",
    "            print('wtf')\n",
    "        \n",
    "        if index == eos:\n",
    "            break\n",
    "\n",
    "        generated_output.append(dec_ind2word.get(index, '<WTF> %s'% index))\n",
    "        dec_start[0][0] = index\n",
    "    \n",
    "    return ' '.join(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input seq: [[  0   7 232]]\n",
      "Input seq: it's cold\n",
      "Output text: estoy bueno.\n"
     ]
    }
   ],
   "source": [
    "print('Output text:', translate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
